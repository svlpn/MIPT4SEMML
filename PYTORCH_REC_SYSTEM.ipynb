{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "pytorch_recomendational_system.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIQbouzuN5fR",
        "outputId": "2218dfe1-d07b-4657-80b1-c626ba8c7d9c"
      },
      "source": [
        "# all config/downloads to use fastai\n",
        "# !pip install pandas --upgrade\n",
        "#!pip install plotly --upgrade\n",
        "#!pip install fastai==0.7.0\n",
        "#!pip install torchtext==0.2.3\n",
        "!pip install torch\n",
        "#!pip install torchvision\n",
        "#!pip install Pillow>=4.1.1\n",
        "#!pip install image\n",
        "#!pip install matplotlib"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gu7n94ogN5fS",
        "outputId": "83ae3d1f-4a50-4fac-ea08-2e300a7fa4d1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/mnt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /mnt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwX4CH-aYclb"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnS13iIyYgbS"
      },
      "source": [
        "data_pd = pd.read_csv('ratings.csv')\n",
        "movies_pd = pd.read_csv('movies.csv')\n",
        "#data_pd = pd.read_csv('ratings.csv') если из локальной папки\n",
        "#movies_pd = pd.read_csv('movies.csv')\n",
        "data_pd = data_pd.sample(frac=1)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07dBCA5_hyL5"
      },
      "source": [
        "u_temp = list(data_pd.userId.unique())\n",
        "u_temp.sort()\n",
        "m_temp = list(data_pd.movieId.unique())\n",
        "m_temp.sort()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCX_KoefjiAS"
      },
      "source": [
        "movie_dict = dict(list(zip(m_temp,range(len(m_temp)))))\n",
        "user_dict = dict(list(zip(u_temp,range(len(u_temp)))))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObuwAPkDZ_Fe"
      },
      "source": [
        "data_pd['userId'] = data_pd['userId'].map(lambda x: user_dict[x])\n",
        "data_pd['movieId'] = data_pd['movieId'].map(lambda x: movie_dict[x])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5h407gkZvyA"
      },
      "source": [
        "X = data_pd[['userId','movieId']].values\n",
        "y = data_pd[['rating']].values"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoW-XZvYN5fU"
      },
      "source": [
        "films = np.transpose(X)[1]\n",
        "counts = np.bincount(films)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "na_mFKUInRp1"
      },
      "source": [
        "y = y.astype(np.float)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wU9d0QZZseE"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVqfwxArTH_k"
      },
      "source": [
        "class RatingDataset():\n",
        "  def __init__(self, train, label):\n",
        "    self.feature_= train\n",
        "    self.label_= label\n",
        "  def __len__(self):\n",
        "    #return size of dataset\n",
        "    return len(self.feature_)\n",
        "  def __getitem__(self, idx):\n",
        "    return torch.tensor(self.feature_[idx]),torch.tensor(self.label_[idx])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJ45RoUpaji6"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTYTBi2oTybM"
      },
      "source": [
        "bs = 5000\n",
        "train_dataloader = DataLoader(RatingDataset(x_train, y_train), batch_size=bs, shuffle=True)\n",
        "test_dataloader = DataLoader(RatingDataset(x_test, y_test), batch_size=bs)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ibe_sZaqTye6"
      },
      "source": [
        "class MatrixFactorization(torch.nn.Module):\n",
        "    \n",
        "    def __init__(self, n_users, n_items, n_factors=20):\n",
        "        super().__init__()\n",
        "        self.user_factors = torch.nn.Embedding(n_users, n_factors)\n",
        "        self.item_factors = torch.nn.Embedding(n_items, n_factors)\n",
        "        self.user_biases = torch.nn.Embedding(n_users, 1)\n",
        "        self.item_biases = torch.nn.Embedding(n_items,1)\n",
        "        torch.nn.init.xavier_uniform_(self.user_factors.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.item_factors.weight)\n",
        "        self.user_biases.weight.data.fill_(0.)\n",
        "        self.item_biases.weight.data.fill_(0.)\n",
        "        \n",
        "    def forward(self, user, item):\n",
        "        pred = self.user_biases(user) + self.item_biases(item)\n",
        "        pred += (self.user_factors(user) * self.item_factors(item)).sum(1, keepdim=True)\n",
        "        #pred = pred.float()\n",
        "        pred.to('cuda')\n",
        "        return pred.squeeze()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAaMTj_La9sl"
      },
      "source": [
        "n, m =  len(data_pd.userId.unique()), len(data_pd.movieId.unique())"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwfzQF1TTyiI"
      },
      "source": [
        "nfactor = 300\n",
        "model = MatrixFactorization(n, m, n_factors=nfactor)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSp-CtZVbXiS"
      },
      "source": [
        "dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "# dev = torch.device(\"cpu\")"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIMYef-aZpMU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "562823c9-39c1-4d77-8b0e-146ca65d7f08"
      },
      "source": [
        "dev"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQdt3LMabZOT",
        "outputId": "a96e49db-6c63-4d38-a747-7c11e2e56f3c"
      },
      "source": [
        "model.parameters"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Module.parameters of MatrixFactorization(\n",
              "  (user_factors): Embedding(610, 300)\n",
              "  (item_factors): Embedding(9724, 300)\n",
              "  (user_biases): Embedding(610, 1)\n",
              "  (item_biases): Embedding(9724, 1)\n",
              ")>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cLD_k_zhPp4",
        "outputId": "88200938-0b56-4eb1-9686-e2c362dc39ad"
      },
      "source": [
        "dev"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpKn6O-chTs4",
        "outputId": "aba9f373-a66b-4b2b-892d-38bede07285e"
      },
      "source": [
        "loss_func = torch.nn.MSELoss()\n",
        "model.to(dev)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MatrixFactorization(\n",
              "  (user_factors): Embedding(610, 300)\n",
              "  (item_factors): Embedding(9724, 300)\n",
              "  (user_biases): Embedding(610, 1)\n",
              "  (item_biases): Embedding(9724, 1)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dx1Zyej08C2z",
        "outputId": "9bf13063-01ee-45cc-cd1c-b2739bd3931f"
      },
      "source": [
        "model.parameters\n",
        "model.double()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MatrixFactorization(\n",
              "  (user_factors): Embedding(610, 300)\n",
              "  (item_factors): Embedding(9724, 300)\n",
              "  (user_biases): Embedding(610, 1)\n",
              "  (item_biases): Embedding(9724, 1)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zGp8094bQTa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70df03ac-17a3-47ba-8ba9-593bff13eebe"
      },
      "source": [
        "epoches = 100\n",
        "train_loss_data = []\n",
        "test_loss_data = []\n",
        "for epoch in range(0, epoches):\n",
        "    pbar = tqdm(enumerate(train_dataloader), total=len(train_dataloader))  # progress bar\n",
        "    count = 0\n",
        "    cum_loss = 0.\n",
        "    for i,( train_batch, label_batch) in pbar:\n",
        "        count = 1 + i\n",
        "        # Predict and calculate loss for user factor and bias\n",
        "        optimizer = torch.optim.SGD([model.user_biases.weight,model.user_factors.weight], lr=0.01, weight_decay=1e-5) # learning rate\n",
        "        prediction = model(train_batch[:,0].to(dev), train_batch[:,1].to(dev))\n",
        "        loss = loss_func(prediction, label_batch.to(dev))    \n",
        "        # Backpropagate\n",
        "        loss.backward()\n",
        "        # Update the parameters\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        #predict and calculate loss for item factor and bias\n",
        "        optimizer = torch.optim.SGD([model.item_biases.weight,model.item_factors.weight], lr=0.01, weight_decay=1e-5) # learning rate\n",
        "        prediction = model(train_batch[:,0].to(dev), train_batch[:,1].to(dev))\n",
        "        loss = loss_func(prediction, label_batch.to(dev))\n",
        "        # Backpropagate\n",
        "        loss.backward()\n",
        "        # Update the parameters\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        cum_loss += loss.item()\n",
        "        pbar.set_description('training loss at {} batch {}: {}'.format(epoch,i,loss.item()))\n",
        "    train_loss = cum_loss/count\n",
        "    pbar = tqdm(enumerate(test_dataloader), total=len(test_dataloader))  # progress bar\n",
        "    cum_loss =0.\n",
        "    count = 0\n",
        "    for i,( test_batch, label_batch) in pbar:\n",
        "        count = 1 + i\n",
        "        with torch.no_grad():\n",
        "            prediction = model(test_batch[:,0].to(dev), test_batch[:,1].to(dev))\n",
        "            loss = loss_func(prediction, label_batch.to(dev))\n",
        "            cum_loss += loss.item()\n",
        "            pbar.set_description('test loss at {} batch {}: {}'.format(epoch,i,loss.item()))\n",
        "    test_loss = cum_loss/count\n",
        "    train_loss_data.append(train_loss)#for graph building\n",
        "    test_loss_data.append(test_loss)\n",
        "    if (np.mean(np.diff(test_loss_data)[:-10]) >= -0.02):\n",
        "      break"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/17 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([5000, 1])) that is different to the input size (torch.Size([5000])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "training loss at 0 batch 15: 13.16664163527037:  88%|████████▊ | 15/17 [00:01<00:00,  8.36it/s] /usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([668, 1])) that is different to the input size (torch.Size([668])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "training loss at 0 batch 16: 13.632511659753243: 100%|██████████| 17/17 [00:01<00:00,  9.80it/s]\n",
            "test loss at 0 batch 3: 13.381215212760162:  80%|████████  | 4/5 [00:00<00:00, 15.22it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([168, 1])) that is different to the input size (torch.Size([168])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "test loss at 0 batch 4: 12.94891434870736: 100%|██████████| 5/5 [00:00<00:00, 18.25it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "training loss at 1 batch 16: 12.863684706672705: 100%|██████████| 17/17 [00:01<00:00, 10.24it/s]\n",
            "test loss at 1 batch 4: 12.834923586646942: 100%|██████████| 5/5 [00:00<00:00, 11.60it/s]\n",
            "training loss at 2 batch 16: 13.129981905029386: 100%|██████████| 17/17 [00:01<00:00, 10.24it/s]\n",
            "test loss at 2 batch 4: 12.723104670733766: 100%|██████████| 5/5 [00:00<00:00, 15.38it/s]\n",
            "training loss at 3 batch 16: 13.140416595520351: 100%|██████████| 17/17 [00:01<00:00, 10.65it/s]\n",
            "test loss at 3 batch 4: 12.613056213617483: 100%|██████████| 5/5 [00:00<00:00, 14.91it/s]\n",
            "training loss at 4 batch 16: 12.900972134718762: 100%|██████████| 17/17 [00:01<00:00, 10.53it/s]\n",
            "test loss at 4 batch 4: 12.505131149720201: 100%|██████████| 5/5 [00:00<00:00, 15.08it/s]\n",
            "training loss at 5 batch 16: 12.668697145073317: 100%|██████████| 17/17 [00:01<00:00, 10.64it/s]\n",
            "test loss at 5 batch 4: 12.399499760329261: 100%|██████████| 5/5 [00:00<00:00, 12.41it/s]\n",
            "training loss at 6 batch 16: 12.631432817619446: 100%|██████████| 17/17 [00:01<00:00, 10.28it/s]\n",
            "test loss at 6 batch 4: 12.295172397667166: 100%|██████████| 5/5 [00:00<00:00, 14.65it/s]\n",
            "training loss at 7 batch 16: 12.73341821969871: 100%|██████████| 17/17 [00:01<00:00,  9.11it/s]\n",
            "test loss at 7 batch 4: 12.192648842818748: 100%|██████████| 5/5 [00:00<00:00, 15.06it/s]\n",
            "training loss at 8 batch 16: 12.099971118546744: 100%|██████████| 17/17 [00:01<00:00, 10.41it/s]\n",
            "test loss at 8 batch 4: 12.092222455801688: 100%|██████████| 5/5 [00:00<00:00, 14.87it/s]\n",
            "training loss at 9 batch 16: 12.16371305519084: 100%|██████████| 17/17 [00:01<00:00,  9.84it/s]\n",
            "test loss at 9 batch 4: 11.993555158489952: 100%|██████████| 5/5 [00:00<00:00, 15.21it/s]\n",
            "training loss at 10 batch 16: 12.112311447956104: 100%|██████████| 17/17 [00:01<00:00, 10.29it/s]\n",
            "test loss at 10 batch 4: 11.896491827058409: 100%|██████████| 5/5 [00:00<00:00, 17.65it/s]\n",
            "training loss at 11 batch 16: 12.485291760830677: 100%|██████████| 17/17 [00:01<00:00, 10.50it/s]\n",
            "test loss at 11 batch 4: 11.800877340012057: 100%|██████████| 5/5 [00:00<00:00, 14.52it/s]\n",
            "training loss at 12 batch 16: 12.636344915639691: 100%|██████████| 17/17 [00:01<00:00, 10.58it/s]\n",
            "test loss at 12 batch 4: 11.707188338002119: 100%|██████████| 5/5 [00:00<00:00, 12.06it/s]\n",
            "training loss at 13 batch 16: 12.138957530500186: 100%|██████████| 17/17 [00:01<00:00, 10.20it/s]\n",
            "test loss at 13 batch 4: 11.61517336950197: 100%|██████████| 5/5 [00:00<00:00, 14.44it/s]\n",
            "training loss at 14 batch 16: 12.091488332865127: 100%|██████████| 17/17 [00:01<00:00,  9.32it/s]\n",
            "test loss at 14 batch 4: 11.524368777212493: 100%|██████████| 5/5 [00:00<00:00, 17.95it/s]\n",
            "training loss at 15 batch 16: 12.056269416544726: 100%|██████████| 17/17 [00:01<00:00, 10.24it/s]\n",
            "test loss at 15 batch 4: 11.435192534604424: 100%|██████████| 5/5 [00:00<00:00, 13.00it/s]\n",
            "training loss at 16 batch 16: 12.012498911578113: 100%|██████████| 17/17 [00:01<00:00, 10.18it/s]\n",
            "test loss at 16 batch 4: 11.34764642184063: 100%|██████████| 5/5 [00:00<00:00, 14.70it/s]\n",
            "training loss at 17 batch 16: 11.613742069942353: 100%|██████████| 17/17 [00:01<00:00, 10.44it/s]\n",
            "test loss at 17 batch 4: 11.261289735133229: 100%|██████████| 5/5 [00:00<00:00, 14.87it/s]\n",
            "training loss at 18 batch 16: 11.734876622659819: 100%|██████████| 17/17 [00:01<00:00,  9.49it/s]\n",
            "test loss at 18 batch 4: 11.176636067906724: 100%|██████████| 5/5 [00:00<00:00, 14.81it/s]\n",
            "training loss at 19 batch 16: 11.884519172951718: 100%|██████████| 17/17 [00:01<00:00, 10.41it/s]\n",
            "test loss at 19 batch 4: 11.09348383371384: 100%|██████████| 5/5 [00:00<00:00, 16.46it/s]\n",
            "training loss at 20 batch 16: 11.374417942645419: 100%|██████████| 17/17 [00:01<00:00, 10.47it/s]\n",
            "test loss at 20 batch 4: 11.011346027975135: 100%|██████████| 5/5 [00:00<00:00, 14.22it/s]\n",
            "training loss at 21 batch 16: 11.218455612186627: 100%|██████████| 17/17 [00:01<00:00,  9.15it/s]\n",
            "test loss at 21 batch 4: 10.930550657942188: 100%|██████████| 5/5 [00:00<00:00, 11.54it/s]\n",
            "training loss at 22 batch 16: 10.828133581767606: 100%|██████████| 17/17 [00:02<00:00,  8.27it/s]\n",
            "test loss at 22 batch 4: 10.850846086214604: 100%|██████████| 5/5 [00:00<00:00, 10.50it/s]\n",
            "training loss at 23 batch 16: 11.096699631352465: 100%|██████████| 17/17 [00:01<00:00,  9.07it/s]\n",
            "test loss at 23 batch 4: 10.77268124584513: 100%|██████████| 5/5 [00:00<00:00, 14.18it/s]\n",
            "training loss at 24 batch 16: 10.880870002289074: 100%|██████████| 17/17 [00:01<00:00,  9.84it/s]\n",
            "test loss at 24 batch 4: 10.695800140157948: 100%|██████████| 5/5 [00:00<00:00, 15.78it/s]\n",
            "training loss at 25 batch 16: 11.411384581100695: 100%|██████████| 17/17 [00:01<00:00, 10.37it/s]\n",
            "test loss at 25 batch 4: 10.619929817842628: 100%|██████████| 5/5 [00:00<00:00, 14.18it/s]\n",
            "training loss at 26 batch 16: 10.95667418205048: 100%|██████████| 17/17 [00:01<00:00, 10.22it/s]\n",
            "test loss at 26 batch 4: 10.54533251171984: 100%|██████████| 5/5 [00:00<00:00, 11.38it/s]\n",
            "training loss at 27 batch 16: 10.785819806145861: 100%|██████████| 17/17 [00:01<00:00,  9.41it/s]\n",
            "test loss at 27 batch 4: 10.471766866469109: 100%|██████████| 5/5 [00:00<00:00, 14.62it/s]\n",
            "training loss at 28 batch 16: 10.904524877161435: 100%|██████████| 17/17 [00:01<00:00,  8.56it/s]\n",
            "test loss at 28 batch 4: 10.399309691633604: 100%|██████████| 5/5 [00:00<00:00, 17.37it/s]\n",
            "training loss at 29 batch 16: 10.716941232939957: 100%|██████████| 17/17 [00:01<00:00,  8.59it/s]\n",
            "test loss at 29 batch 4: 10.327942553845672: 100%|██████████| 5/5 [00:00<00:00, 14.09it/s]\n",
            "training loss at 30 batch 16: 10.749583412826702: 100%|██████████| 17/17 [00:01<00:00,  9.94it/s]\n",
            "test loss at 30 batch 4: 10.257663141817746: 100%|██████████| 5/5 [00:00<00:00, 13.85it/s]\n",
            "training loss at 31 batch 16: 10.627804029008843: 100%|██████████| 17/17 [00:01<00:00, 10.25it/s]\n",
            "test loss at 31 batch 4: 10.188272483834842: 100%|██████████| 5/5 [00:00<00:00, 13.48it/s]\n",
            "training loss at 32 batch 16: 10.216589382634185: 100%|██████████| 17/17 [00:01<00:00, 10.39it/s]\n",
            "test loss at 32 batch 4: 10.119976766771694: 100%|██████████| 5/5 [00:00<00:00, 11.47it/s]\n",
            "training loss at 33 batch 16: 10.131564947937388: 100%|██████████| 17/17 [00:01<00:00, 10.05it/s]\n",
            "test loss at 33 batch 4: 10.052757358326568: 100%|██████████| 5/5 [00:00<00:00, 16.83it/s]\n",
            "training loss at 34 batch 16: 10.620876197154592: 100%|██████████| 17/17 [00:01<00:00,  8.91it/s]\n",
            "test loss at 34 batch 4: 9.986301546013145: 100%|██████████| 5/5 [00:00<00:00, 13.46it/s]\n",
            "training loss at 35 batch 16: 10.555834868651118: 100%|██████████| 17/17 [00:01<00:00,  9.84it/s]\n",
            "test loss at 35 batch 4: 9.920790826394192: 100%|██████████| 5/5 [00:00<00:00, 14.32it/s]\n",
            "training loss at 36 batch 16: 10.34196568875278: 100%|██████████| 17/17 [00:01<00:00,  8.70it/s]\n",
            "test loss at 36 batch 4: 9.856068709314458: 100%|██████████| 5/5 [00:00<00:00, 10.43it/s]\n",
            "training loss at 37 batch 16: 10.511638954292605: 100%|██████████| 17/17 [00:01<00:00, 10.21it/s]\n",
            "test loss at 37 batch 4: 9.792523578519276: 100%|██████████| 5/5 [00:00<00:00, 12.03it/s]\n",
            "training loss at 38 batch 16: 10.144520933795617: 100%|██████████| 17/17 [00:01<00:00, 10.04it/s]\n",
            "test loss at 38 batch 4: 9.729854242131163: 100%|██████████| 5/5 [00:00<00:00, 16.97it/s]\n",
            "training loss at 39 batch 16: 10.109094783413424: 100%|██████████| 17/17 [00:01<00:00, 10.19it/s]\n",
            "test loss at 39 batch 4: 9.668038615444328: 100%|██████████| 5/5 [00:00<00:00, 12.18it/s]\n",
            "training loss at 40 batch 16: 9.917427366567942: 100%|██████████| 17/17 [00:01<00:00,  9.18it/s]\n",
            "test loss at 40 batch 4: 9.607056783826499: 100%|██████████| 5/5 [00:00<00:00,  9.64it/s]\n",
            "training loss at 41 batch 16: 10.332448369248: 100%|██████████| 17/17 [00:01<00:00, 10.02it/s]\n",
            "test loss at 41 batch 4: 9.54683312847332: 100%|██████████| 5/5 [00:00<00:00, 14.26it/s]\n",
            "training loss at 42 batch 16: 10.312728280554056: 100%|██████████| 17/17 [00:01<00:00,  9.82it/s]\n",
            "test loss at 42 batch 4: 9.487401067031687: 100%|██████████| 5/5 [00:00<00:00, 17.05it/s]\n",
            "training loss at 43 batch 16: 9.903433801314366: 100%|██████████| 17/17 [00:01<00:00,  9.63it/s]\n",
            "test loss at 43 batch 4: 9.4288071908002: 100%|██████████| 5/5 [00:00<00:00, 14.24it/s]\n",
            "training loss at 44 batch 16: 9.766708103550235: 100%|██████████| 17/17 [00:01<00:00,  8.89it/s]\n",
            "test loss at 44 batch 4: 9.370937875984238: 100%|██████████| 5/5 [00:00<00:00, 10.11it/s]\n",
            "training loss at 45 batch 16: 9.607089924975696: 100%|██████████| 17/17 [00:01<00:00,  9.18it/s]\n",
            "test loss at 45 batch 4: 9.313835365174102: 100%|██████████| 5/5 [00:00<00:00, 14.23it/s]\n",
            "training loss at 46 batch 16: 9.48373029950184: 100%|██████████| 17/17 [00:01<00:00,  9.51it/s]\n",
            "test loss at 46 batch 4: 9.257472892583916: 100%|██████████| 5/5 [00:00<00:00,  9.34it/s]\n",
            "training loss at 47 batch 16: 9.605737891442825: 100%|██████████| 17/17 [00:01<00:00,  9.95it/s]\n",
            "test loss at 47 batch 4: 9.201927170340866: 100%|██████████| 5/5 [00:00<00:00, 16.30it/s]\n",
            "training loss at 48 batch 16: 9.674479858843188: 100%|██████████| 17/17 [00:01<00:00, 10.11it/s]\n",
            "test loss at 48 batch 4: 9.147172547107308: 100%|██████████| 5/5 [00:00<00:00, 12.63it/s]\n",
            "training loss at 49 batch 16: 9.508656620118014: 100%|██████████| 17/17 [00:01<00:00,  9.76it/s]\n",
            "test loss at 49 batch 4: 9.09294715242754: 100%|██████████| 5/5 [00:00<00:00, 13.77it/s]\n",
            "training loss at 50 batch 16: 9.648132649051313: 100%|██████████| 17/17 [00:01<00:00,  9.75it/s]\n",
            "test loss at 50 batch 4: 9.039378997406878: 100%|██████████| 5/5 [00:00<00:00, 13.30it/s]\n",
            "training loss at 51 batch 16: 9.526141192981514: 100%|██████████| 17/17 [00:01<00:00,  9.83it/s]\n",
            "test loss at 51 batch 4: 8.98668296825483: 100%|██████████| 5/5 [00:00<00:00, 13.88it/s]\n",
            "training loss at 52 batch 16: 9.243729651039384: 100%|██████████| 17/17 [00:01<00:00,  9.86it/s]\n",
            "test loss at 52 batch 4: 8.934561031872937: 100%|██████████| 5/5 [00:00<00:00, 16.82it/s]\n",
            "training loss at 53 batch 16: 9.349772922947217: 100%|██████████| 17/17 [00:01<00:00,  9.68it/s]\n",
            "test loss at 53 batch 4: 8.88291393668001: 100%|██████████| 5/5 [00:00<00:00, 13.61it/s]\n",
            "training loss at 54 batch 16: 9.391337856262863: 100%|██████████| 17/17 [00:01<00:00,  9.81it/s]\n",
            "test loss at 54 batch 4: 8.831926815660244: 100%|██████████| 5/5 [00:00<00:00, 13.87it/s]\n",
            "training loss at 55 batch 16: 9.108788360053444: 100%|██████████| 17/17 [00:01<00:00,  9.82it/s]\n",
            "test loss at 55 batch 4: 8.781525709434753: 100%|██████████| 5/5 [00:00<00:00, 13.21it/s]\n",
            "training loss at 56 batch 16: 9.175661945318128: 100%|██████████| 17/17 [00:01<00:00,  9.69it/s]\n",
            "test loss at 56 batch 4: 8.732028643495216: 100%|██████████| 5/5 [00:00<00:00, 14.11it/s]\n",
            "training loss at 57 batch 16: 9.245840172146671: 100%|██████████| 17/17 [00:01<00:00,  9.09it/s]\n",
            "test loss at 57 batch 4: 8.683092495443228: 100%|██████████| 5/5 [00:00<00:00, 13.22it/s]\n",
            "training loss at 58 batch 16: 9.317961121619838: 100%|██████████| 17/17 [00:01<00:00,  9.99it/s]\n",
            "test loss at 58 batch 4: 8.634730341953471: 100%|██████████| 5/5 [00:00<00:00,  9.59it/s]\n",
            "training loss at 59 batch 16: 9.639835675389872: 100%|██████████| 17/17 [00:01<00:00,  9.63it/s]\n",
            "test loss at 59 batch 4: 8.586732294091986: 100%|██████████| 5/5 [00:00<00:00, 13.89it/s]\n",
            "training loss at 60 batch 16: 8.986066506924525: 100%|██████████| 17/17 [00:01<00:00,  9.13it/s]\n",
            "test loss at 60 batch 4: 8.539437418117098: 100%|██████████| 5/5 [00:00<00:00, 13.30it/s]\n",
            "training loss at 61 batch 16: 9.187559060968935: 100%|██████████| 17/17 [00:01<00:00,  9.69it/s]\n",
            "test loss at 61 batch 4: 8.492728307242057: 100%|██████████| 5/5 [00:00<00:00, 15.75it/s]\n",
            "training loss at 62 batch 16: 9.745847049348614: 100%|██████████| 17/17 [00:01<00:00,  9.63it/s]\n",
            "test loss at 62 batch 4: 8.446355873845924: 100%|██████████| 5/5 [00:00<00:00, 11.32it/s]\n",
            "training loss at 63 batch 16: 8.724383448148354: 100%|██████████| 17/17 [00:01<00:00, 10.00it/s]\n",
            "test loss at 63 batch 4: 8.40060403940217: 100%|██████████| 5/5 [00:00<00:00, 13.74it/s]\n",
            "training loss at 64 batch 16: 8.684161816240039: 100%|██████████| 17/17 [00:01<00:00,  8.80it/s]\n",
            "test loss at 64 batch 4: 8.355506093647705: 100%|██████████| 5/5 [00:00<00:00, 13.34it/s]\n",
            "training loss at 65 batch 16: 8.811903253280317: 100%|██████████| 17/17 [00:01<00:00,  9.67it/s]\n",
            "test loss at 65 batch 4: 8.310611748606098: 100%|██████████| 5/5 [00:00<00:00, 13.67it/s]\n",
            "training loss at 66 batch 16: 8.802501293861745: 100%|██████████| 17/17 [00:01<00:00,  9.91it/s]\n",
            "test loss at 66 batch 4: 8.2663355087959: 100%|██████████| 5/5 [00:00<00:00, 16.88it/s]\n",
            "training loss at 67 batch 16: 8.665661256568526: 100%|██████████| 17/17 [00:01<00:00,  8.53it/s]\n",
            "test loss at 67 batch 4: 8.222719809643687: 100%|██████████| 5/5 [00:00<00:00, 13.09it/s]\n",
            "training loss at 68 batch 16: 8.975303276069567: 100%|██████████| 17/17 [00:01<00:00,  9.92it/s]\n",
            "test loss at 68 batch 4: 8.179629307973789: 100%|██████████| 5/5 [00:00<00:00, 13.51it/s]\n",
            "training loss at 69 batch 16: 8.699725038424022: 100%|██████████| 17/17 [00:01<00:00,  9.71it/s]\n",
            "test loss at 69 batch 4: 8.137075180194094: 100%|██████████| 5/5 [00:00<00:00, 13.78it/s]\n",
            "training loss at 70 batch 16: 8.855593914314165: 100%|██████████| 17/17 [00:01<00:00,  9.91it/s]\n",
            "test loss at 70 batch 4: 8.095066242247176: 100%|██████████| 5/5 [00:00<00:00, 16.27it/s]\n",
            "training loss at 71 batch 16: 8.579134634365529: 100%|██████████| 17/17 [00:01<00:00,  9.31it/s]\n",
            "test loss at 71 batch 4: 8.053370648769603: 100%|██████████| 5/5 [00:00<00:00, 13.39it/s]\n",
            "training loss at 72 batch 16: 8.317238387849489: 100%|██████████| 17/17 [00:01<00:00,  9.40it/s]\n",
            "test loss at 72 batch 4: 8.012196992134829: 100%|██████████| 5/5 [00:00<00:00, 14.04it/s]\n",
            "training loss at 73 batch 16: 8.77293517386255: 100%|██████████| 17/17 [00:01<00:00,  9.54it/s]\n",
            "test loss at 73 batch 4: 7.971424814352468: 100%|██████████| 5/5 [00:00<00:00, 12.71it/s]\n",
            "training loss at 74 batch 16: 8.678507563214591: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s]\n",
            "test loss at 74 batch 4: 7.931023805640665: 100%|██████████| 5/5 [00:00<00:00, 13.95it/s]\n",
            "training loss at 75 batch 16: 8.092773441382255: 100%|██████████| 17/17 [00:01<00:00,  9.41it/s]\n",
            "test loss at 75 batch 4: 7.891089674899607: 100%|██████████| 5/5 [00:00<00:00, 10.77it/s]\n",
            "training loss at 76 batch 16: 8.583920349733411: 100%|██████████| 17/17 [00:01<00:00, 10.05it/s]\n",
            "test loss at 76 batch 4: 7.851570432643512: 100%|██████████| 5/5 [00:00<00:00, 13.78it/s]\n",
            "training loss at 77 batch 16: 8.21050118983997: 100%|██████████| 17/17 [00:01<00:00,  8.78it/s]\n",
            "test loss at 77 batch 4: 7.812546383637549: 100%|██████████| 5/5 [00:00<00:00, 13.81it/s]\n",
            "training loss at 78 batch 16: 8.419428191777063: 100%|██████████| 17/17 [00:01<00:00,  9.34it/s]\n",
            "test loss at 78 batch 4: 7.773924548401696: 100%|██████████| 5/5 [00:00<00:00, 12.41it/s]\n",
            "training loss at 79 batch 16: 8.364928972929427: 100%|██████████| 17/17 [00:01<00:00,  8.61it/s]\n",
            "test loss at 79 batch 4: 7.735807456995616: 100%|██████████| 5/5 [00:00<00:00, 13.08it/s]\n",
            "training loss at 80 batch 16: 7.92639111876803: 100%|██████████| 17/17 [00:01<00:00,  9.52it/s]\n",
            "test loss at 80 batch 4: 7.69799414401685: 100%|██████████| 5/5 [00:00<00:00, 14.76it/s]\n",
            "training loss at 81 batch 16: 7.832687246924005: 100%|██████████| 17/17 [00:01<00:00,  8.69it/s]\n",
            "test loss at 81 batch 4: 7.6606259707174775: 100%|██████████| 5/5 [00:00<00:00, 13.50it/s]\n",
            "training loss at 82 batch 16: 7.936008270598514: 100%|██████████| 17/17 [00:02<00:00,  8.47it/s]\n",
            "test loss at 82 batch 4: 7.6235281224815: 100%|██████████| 5/5 [00:00<00:00, 13.91it/s]\n",
            "training loss at 83 batch 16: 8.152804277587698: 100%|██████████| 17/17 [00:01<00:00,  8.90it/s]\n",
            "test loss at 83 batch 4: 7.586817711909145: 100%|██████████| 5/5 [00:00<00:00, 13.45it/s]\n",
            "training loss at 84 batch 16: 8.071250146661821: 100%|██████████| 17/17 [00:01<00:00,  9.92it/s]\n",
            "test loss at 84 batch 4: 7.55053695783746: 100%|██████████| 5/5 [00:00<00:00, 15.76it/s]\n",
            "training loss at 85 batch 16: 7.654786825508268: 100%|██████████| 17/17 [00:02<00:00,  8.49it/s]\n",
            "test loss at 85 batch 4: 7.514591789783355: 100%|██████████| 5/5 [00:00<00:00, 13.28it/s]\n",
            "training loss at 86 batch 16: 7.828311001323372: 100%|██████████| 17/17 [00:01<00:00,  8.97it/s]\n",
            "test loss at 86 batch 4: 7.478956913388873: 100%|██████████| 5/5 [00:00<00:00, 13.96it/s]\n",
            "training loss at 87 batch 16: 8.102316426331098: 100%|██████████| 17/17 [00:01<00:00,  9.65it/s]\n",
            "test loss at 87 batch 4: 7.443674667189649: 100%|██████████| 5/5 [00:00<00:00, 13.54it/s]\n",
            "training loss at 88 batch 16: 7.67674831036483: 100%|██████████| 17/17 [00:01<00:00, 10.07it/s]\n",
            "test loss at 88 batch 4: 7.4087506780715024: 100%|██████████| 5/5 [00:00<00:00, 13.62it/s]\n",
            "training loss at 89 batch 16: 7.784153480654485: 100%|██████████| 17/17 [00:01<00:00,  9.89it/s]\n",
            "test loss at 89 batch 4: 7.3741073942338256: 100%|██████████| 5/5 [00:00<00:00, 16.42it/s]\n",
            "training loss at 90 batch 16: 7.588392410101054: 100%|██████████| 17/17 [00:01<00:00,  9.27it/s]\n",
            "test loss at 90 batch 4: 7.3398203097453205: 100%|██████████| 5/5 [00:00<00:00, 12.00it/s]\n",
            "training loss at 91 batch 16: 7.651920423393238: 100%|██████████| 17/17 [00:01<00:00,  9.99it/s]\n",
            "test loss at 91 batch 4: 7.30593634829969: 100%|██████████| 5/5 [00:00<00:00, 13.84it/s]\n",
            "training loss at 92 batch 16: 7.3656810721705375: 100%|██████████| 17/17 [00:01<00:00,  9.62it/s]\n",
            "test loss at 92 batch 4: 7.272420431788067: 100%|██████████| 5/5 [00:00<00:00, 11.52it/s]\n",
            "training loss at 93 batch 16: 7.752912369320853: 100%|██████████| 17/17 [00:01<00:00, 10.06it/s]\n",
            "test loss at 93 batch 4: 7.23921265173373: 100%|██████████| 5/5 [00:00<00:00, 13.45it/s]\n",
            "training loss at 94 batch 16: 7.714467911104236: 100%|██████████| 17/17 [00:01<00:00,  9.90it/s]\n",
            "test loss at 94 batch 4: 7.2063579618663605: 100%|██████████| 5/5 [00:00<00:00, 16.06it/s]\n",
            "training loss at 95 batch 16: 7.398434688453496: 100%|██████████| 17/17 [00:01<00:00,  9.67it/s]\n",
            "test loss at 95 batch 4: 7.173783355024221: 100%|██████████| 5/5 [00:00<00:00, 13.24it/s]\n",
            "training loss at 96 batch 16: 7.478567880400686: 100%|██████████| 17/17 [00:01<00:00,  9.85it/s]\n",
            "test loss at 96 batch 4: 7.141426450191982: 100%|██████████| 5/5 [00:00<00:00, 13.75it/s]\n",
            "training loss at 97 batch 16: 7.613373781804966: 100%|██████████| 17/17 [00:01<00:00,  9.02it/s]\n",
            "test loss at 97 batch 4: 7.109371608987376: 100%|██████████| 5/5 [00:00<00:00, 12.66it/s]\n",
            "training loss at 98 batch 16: 7.776144336272584: 100%|██████████| 17/17 [00:01<00:00,  9.65it/s]\n",
            "test loss at 98 batch 4: 7.077737267201307: 100%|██████████| 5/5 [00:00<00:00, 13.98it/s]\n",
            "training loss at 99 batch 16: 7.131614405690287: 100%|██████████| 17/17 [00:01<00:00,  9.93it/s]\n",
            "test loss at 99 batch 4: 7.046413054427367: 100%|██████████| 5/5 [00:00<00:00, 13.62it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLQmEyScnSwu"
      },
      "source": [
        "movie_id = movie_dict[random.randint(1, 20)] # random film\n",
        "sim_dict = {} #films similarity\n",
        "for m, id in zip(model.item_factors.weight.data.cpu().numpy(), movie_dict.keys()): #cos metric\n",
        "    sim_dict[id] = np.dot(model.item_factors.weight.data.cpu().numpy()[movie_id], m)/(np.linalg.norm(model.item_factors.weight.data.cpu().numpy()[movie_id])*np.linalg.norm(m))"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQRm0OAjaucT"
      },
      "source": [
        "sim_pd = pd.DataFrame(sim_dict.items(), columns=['movieId', 'similarity']).merge(movies_pd, on='movieId')"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JsAqqhMkPMH"
      },
      "source": [
        "Films, similar to \"Matrix\":"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        },
        "id": "hJngu1KQbqGL",
        "scrolled": true,
        "outputId": "265f9214-30d1-44e3-b76b-dcf5ab556405"
      },
      "source": [
        "sim_pd.sort_values('similarity', ascending=False)[0:30][['title', 'genres', 'similarity']]"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>genres</th>\n",
              "      <th>similarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Money Train (1995)</td>\n",
              "      <td>Action|Comedy|Crime|Drama|Thriller</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4184</th>\n",
              "      <td>Amen. (2002)</td>\n",
              "      <td>Drama</td>\n",
              "      <td>0.193525</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3134</th>\n",
              "      <td>Exit Wounds (2001)</td>\n",
              "      <td>Action|Thriller</td>\n",
              "      <td>0.186160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1102</th>\n",
              "      <td>Metro (1997)</td>\n",
              "      <td>Action|Comedy|Crime|Drama|Thriller</td>\n",
              "      <td>0.185538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3682</th>\n",
              "      <td>Cousins (1989)</td>\n",
              "      <td>Comedy|Romance</td>\n",
              "      <td>0.183668</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2112</th>\n",
              "      <td>Universal Soldier: The Return (1999)</td>\n",
              "      <td>Action|Sci-Fi</td>\n",
              "      <td>0.181719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5739</th>\n",
              "      <td>Sweet Liberty (1986)</td>\n",
              "      <td>Comedy</td>\n",
              "      <td>0.178442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3229</th>\n",
              "      <td>A.I. Artificial Intelligence (2001)</td>\n",
              "      <td>Adventure|Drama|Sci-Fi</td>\n",
              "      <td>0.177547</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5039</th>\n",
              "      <td>Duck, You Sucker (1971)</td>\n",
              "      <td>Action|Western</td>\n",
              "      <td>0.174478</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3805</th>\n",
              "      <td>Cadillac Man (1990)</td>\n",
              "      <td>Comedy|Crime</td>\n",
              "      <td>0.174027</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6647</th>\n",
              "      <td>Cloverfield (2008)</td>\n",
              "      <td>Action|Mystery|Sci-Fi|Thriller</td>\n",
              "      <td>0.172799</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8611</th>\n",
              "      <td>Wild (2014)</td>\n",
              "      <td>Drama</td>\n",
              "      <td>0.165830</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4891</th>\n",
              "      <td>Agent Cody Banks 2: Destination London (2004)</td>\n",
              "      <td>Action|Adventure|Children|Comedy</td>\n",
              "      <td>0.164525</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5621</th>\n",
              "      <td>Monday (2000)</td>\n",
              "      <td>Action|Comedy|Crime|Fantasy|Thriller</td>\n",
              "      <td>0.163787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>860</th>\n",
              "      <td>Manon of the Spring (Manon des sources) (1986)</td>\n",
              "      <td>Drama</td>\n",
              "      <td>0.163308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5474</th>\n",
              "      <td>Emmanuelle (1974)</td>\n",
              "      <td>Drama|Romance</td>\n",
              "      <td>0.163183</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8160</th>\n",
              "      <td>After Earth (2013)</td>\n",
              "      <td>Action|Adventure|Sci-Fi|IMAX</td>\n",
              "      <td>0.162481</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2429</th>\n",
              "      <td>Encino Man (1992)</td>\n",
              "      <td>Comedy</td>\n",
              "      <td>0.162174</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1753</th>\n",
              "      <td>Big Chill, The (1983)</td>\n",
              "      <td>Comedy|Drama</td>\n",
              "      <td>0.160651</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>889</th>\n",
              "      <td>Passion Fish (1992)</td>\n",
              "      <td>Drama</td>\n",
              "      <td>0.160026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9081</th>\n",
              "      <td>Er ist wieder da (2015)</td>\n",
              "      <td>Comedy</td>\n",
              "      <td>0.158993</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6002</th>\n",
              "      <td>All This, and Heaven Too (1940)</td>\n",
              "      <td>Drama|Romance</td>\n",
              "      <td>0.158230</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>993</th>\n",
              "      <td>Room with a View, A (1986)</td>\n",
              "      <td>Drama|Romance</td>\n",
              "      <td>0.157914</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6693</th>\n",
              "      <td>Dark Knight, The (2008)</td>\n",
              "      <td>Action|Crime|Drama|IMAX</td>\n",
              "      <td>0.156820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7675</th>\n",
              "      <td>Avengers, The (2012)</td>\n",
              "      <td>Action|Adventure|Sci-Fi|IMAX</td>\n",
              "      <td>0.155159</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>241</th>\n",
              "      <td>My Family (1995)</td>\n",
              "      <td>Drama</td>\n",
              "      <td>0.154639</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2120</th>\n",
              "      <td>Iron Eagle IV (1995)</td>\n",
              "      <td>Action|War</td>\n",
              "      <td>0.154473</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4500</th>\n",
              "      <td>Ikiru (1952)</td>\n",
              "      <td>Drama</td>\n",
              "      <td>0.153608</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4438</th>\n",
              "      <td>Masked &amp; Anonymous (2003)</td>\n",
              "      <td>Comedy|Drama</td>\n",
              "      <td>0.153376</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2064</th>\n",
              "      <td>Otello (1986)</td>\n",
              "      <td>Drama</td>\n",
              "      <td>0.153282</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ... similarity\n",
              "19                                Money Train (1995)  ...   1.000000\n",
              "4184                                    Amen. (2002)  ...   0.193525\n",
              "3134                              Exit Wounds (2001)  ...   0.186160\n",
              "1102                                    Metro (1997)  ...   0.185538\n",
              "3682                                  Cousins (1989)  ...   0.183668\n",
              "2112            Universal Soldier: The Return (1999)  ...   0.181719\n",
              "5739                            Sweet Liberty (1986)  ...   0.178442\n",
              "3229             A.I. Artificial Intelligence (2001)  ...   0.177547\n",
              "5039                         Duck, You Sucker (1971)  ...   0.174478\n",
              "3805                             Cadillac Man (1990)  ...   0.174027\n",
              "6647                              Cloverfield (2008)  ...   0.172799\n",
              "8611                                     Wild (2014)  ...   0.165830\n",
              "4891   Agent Cody Banks 2: Destination London (2004)  ...   0.164525\n",
              "5621                                   Monday (2000)  ...   0.163787\n",
              "860   Manon of the Spring (Manon des sources) (1986)  ...   0.163308\n",
              "5474                               Emmanuelle (1974)  ...   0.163183\n",
              "8160                              After Earth (2013)  ...   0.162481\n",
              "2429                               Encino Man (1992)  ...   0.162174\n",
              "1753                           Big Chill, The (1983)  ...   0.160651\n",
              "889                              Passion Fish (1992)  ...   0.160026\n",
              "9081                         Er ist wieder da (2015)  ...   0.158993\n",
              "6002                 All This, and Heaven Too (1940)  ...   0.158230\n",
              "993                       Room with a View, A (1986)  ...   0.157914\n",
              "6693                         Dark Knight, The (2008)  ...   0.156820\n",
              "7675                            Avengers, The (2012)  ...   0.155159\n",
              "241                                 My Family (1995)  ...   0.154639\n",
              "2120                            Iron Eagle IV (1995)  ...   0.154473\n",
              "4500                                    Ikiru (1952)  ...   0.153608\n",
              "4438                       Masked & Anonymous (2003)  ...   0.153376\n",
              "2064                                   Otello (1986)  ...   0.153282\n",
              "\n",
              "[30 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9-3ZjLCN5fZ"
      },
      "source": [
        "import random\n",
        "userId = random.randint(1, 20) #check of first 20 persons randomly"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JisGvuRPx1k"
      },
      "source": [
        "ratings_dict = {}\n",
        "user_bias = model.user_biases.weight.data.cpu().numpy()[userId][0]\n",
        "for pred, item_bias, id in zip(model.item_factors.weight.data.cpu().numpy().dot(model.user_factors.weight.data.cpu().numpy()[userId]),\n",
        "model.item_biases.weight.data.cpu().numpy(), movie_dict.keys()):\n",
        "  if not id in np.transpose(data_pd[data_pd['userId'] == user_dict[42]][['movieId']].to_numpy())[0]:\n",
        "    ratings_dict[id] = pred + item_bias[0] + user_bias"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLJOdilaP0Ks"
      },
      "source": [
        "ratings_pd = pd.DataFrame(ratings_dict.items(), columns=['movieId', 'rating'])\n",
        "ratings_pd = ratings_pd.merge(movies_pd, on='movieId')"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmqO-Rxdkzbg"
      },
      "source": [
        "Best 30 films for random user:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        },
        "id": "lmPn1QjUP3Q7",
        "outputId": "aef7a4ad-44e0-42f9-eaed-5a39dc03e869"
      },
      "source": [
        "ratings_pd.sort_values('rating', ascending=False)[0:30][['title', 'genres', 'rating']]"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>genres</th>\n",
              "      <th>rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>259</th>\n",
              "      <td>Forrest Gump (1994)</td>\n",
              "      <td>Comedy|Drama|Romance|War</td>\n",
              "      <td>0.752453</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1662</th>\n",
              "      <td>Matrix, The (1999)</td>\n",
              "      <td>Action|Sci-Fi|Thriller</td>\n",
              "      <td>0.682532</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>218</th>\n",
              "      <td>Pulp Fiction (1994)</td>\n",
              "      <td>Comedy|Crime|Drama|Thriller</td>\n",
              "      <td>0.680423</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>418</th>\n",
              "      <td>Silence of the Lambs, The (1991)</td>\n",
              "      <td>Crime|Horror|Thriller</td>\n",
              "      <td>0.658688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>189</th>\n",
              "      <td>Star Wars: Episode IV - A New Hope (1977)</td>\n",
              "      <td>Action|Adventure|Sci-Fi</td>\n",
              "      <td>0.614926</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>374</th>\n",
              "      <td>Schindler's List (1993)</td>\n",
              "      <td>Drama|War</td>\n",
              "      <td>0.571937</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Toy Story (1995)</td>\n",
              "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
              "      <td>0.562603</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83</th>\n",
              "      <td>Braveheart (1995)</td>\n",
              "      <td>Action|Drama|War</td>\n",
              "      <td>0.555509</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>416</th>\n",
              "      <td>Terminator 2: Judgment Day (1991)</td>\n",
              "      <td>Action|Sci-Fi</td>\n",
              "      <td>0.543097</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>756</th>\n",
              "      <td>Star Wars: Episode V - The Empire Strikes Back...</td>\n",
              "      <td>Action|Adventure|Sci-Fi</td>\n",
              "      <td>0.537720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1855</th>\n",
              "      <td>American Beauty (1999)</td>\n",
              "      <td>Drama|Romance</td>\n",
              "      <td>0.519799</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>Apollo 13 (1995)</td>\n",
              "      <td>Adventure|Drama|IMAX</td>\n",
              "      <td>0.516013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>415</th>\n",
              "      <td>Aladdin (1992)</td>\n",
              "      <td>Adventure|Animation|Children|Comedy|Musical</td>\n",
              "      <td>0.515827</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>Usual Suspects, The (1995)</td>\n",
              "      <td>Crime|Mystery|Thriller</td>\n",
              "      <td>0.515536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1931</th>\n",
              "      <td>Fight Club (1999)</td>\n",
              "      <td>Action|Crime|Drama|Thriller</td>\n",
              "      <td>0.511270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3298</th>\n",
              "      <td>Lord of the Rings: The Fellowship of the Ring,...</td>\n",
              "      <td>Adventure|Fantasy</td>\n",
              "      <td>0.507531</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>Seven (a.k.a. Se7en) (1995)</td>\n",
              "      <td>Mystery|Thriller</td>\n",
              "      <td>0.506338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>514</th>\n",
              "      <td>Independence Day (a.k.a. ID4) (1996)</td>\n",
              "      <td>Action|Adventure|Sci-Fi|Thriller</td>\n",
              "      <td>0.499780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4456</th>\n",
              "      <td>Lord of the Rings: The Return of the King, The...</td>\n",
              "      <td>Action|Adventure|Drama|Fantasy</td>\n",
              "      <td>0.493052</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>549</th>\n",
              "      <td>Godfather, The (1972)</td>\n",
              "      <td>Crime|Drama</td>\n",
              "      <td>0.491272</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3796</th>\n",
              "      <td>Lord of the Rings: The Two Towers, The (2002)</td>\n",
              "      <td>Adventure|Fantasy</td>\n",
              "      <td>0.487004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2335</th>\n",
              "      <td>Gladiator (2000)</td>\n",
              "      <td>Action|Adventure|Drama</td>\n",
              "      <td>0.478254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1793</th>\n",
              "      <td>Sixth Sense, The (1999)</td>\n",
              "      <td>Drama|Horror|Mystery</td>\n",
              "      <td>0.474654</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249</th>\n",
              "      <td>Ace Ventura: Pet Detective (1994)</td>\n",
              "      <td>Comedy</td>\n",
              "      <td>0.471504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1278</th>\n",
              "      <td>Saving Private Ryan (1998)</td>\n",
              "      <td>Action|Drama|War</td>\n",
              "      <td>0.467772</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>428</th>\n",
              "      <td>Fargo (1996)</td>\n",
              "      <td>Comedy|Crime|Drama|Thriller</td>\n",
              "      <td>0.464941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>Twelve Monkeys (a.k.a. 12 Monkeys) (1995)</td>\n",
              "      <td>Mystery|Sci-Fi|Thriller</td>\n",
              "      <td>0.461064</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>810</th>\n",
              "      <td>Back to the Future (1985)</td>\n",
              "      <td>Adventure|Comedy|Sci-Fi</td>\n",
              "      <td>0.457637</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>325</th>\n",
              "      <td>Fugitive, The (1993)</td>\n",
              "      <td>Thriller</td>\n",
              "      <td>0.455953</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>450</th>\n",
              "      <td>Mission: Impossible (1996)</td>\n",
              "      <td>Action|Adventure|Mystery|Thriller</td>\n",
              "      <td>0.433249</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  title  ...    rating\n",
              "259                                 Forrest Gump (1994)  ...  0.752453\n",
              "1662                                 Matrix, The (1999)  ...  0.682532\n",
              "218                                 Pulp Fiction (1994)  ...  0.680423\n",
              "418                    Silence of the Lambs, The (1991)  ...  0.658688\n",
              "189           Star Wars: Episode IV - A New Hope (1977)  ...  0.614926\n",
              "374                             Schindler's List (1993)  ...  0.571937\n",
              "0                                      Toy Story (1995)  ...  0.562603\n",
              "83                                    Braveheart (1995)  ...  0.555509\n",
              "416                   Terminator 2: Judgment Day (1991)  ...  0.543097\n",
              "756   Star Wars: Episode V - The Empire Strikes Back...  ...  0.537720\n",
              "1855                             American Beauty (1999)  ...  0.519799\n",
              "102                                    Apollo 13 (1995)  ...  0.516013\n",
              "415                                      Aladdin (1992)  ...  0.515827\n",
              "36                           Usual Suspects, The (1995)  ...  0.515536\n",
              "1931                                  Fight Club (1999)  ...  0.511270\n",
              "3298  Lord of the Rings: The Fellowship of the Ring,...  ...  0.507531\n",
              "33                          Seven (a.k.a. Se7en) (1995)  ...  0.506338\n",
              "514                Independence Day (a.k.a. ID4) (1996)  ...  0.499780\n",
              "4456  Lord of the Rings: The Return of the King, The...  ...  0.493052\n",
              "549                               Godfather, The (1972)  ...  0.491272\n",
              "3796      Lord of the Rings: The Two Towers, The (2002)  ...  0.487004\n",
              "2335                                   Gladiator (2000)  ...  0.478254\n",
              "1793                            Sixth Sense, The (1999)  ...  0.474654\n",
              "249                   Ace Ventura: Pet Detective (1994)  ...  0.471504\n",
              "1278                         Saving Private Ryan (1998)  ...  0.467772\n",
              "428                                        Fargo (1996)  ...  0.464941\n",
              "23            Twelve Monkeys (a.k.a. 12 Monkeys) (1995)  ...  0.461064\n",
              "810                           Back to the Future (1985)  ...  0.457637\n",
              "325                                Fugitive, The (1993)  ...  0.455953\n",
              "450                          Mission: Impossible (1996)  ...  0.433249\n",
              "\n",
              "[30 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghJH_3LS2h4q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}